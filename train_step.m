function [loss, y, W1, b1, W2, b2] = train_step(train_data, train_label, ...
                    W1, b1, ...
                    W2, b2, ...
                    lr)
% 一层隐藏层人工神经网络单步训练的实现
%
% :param train_data: 输入训练样本块
% :param train_label: 输入训练样本标签
% :param W1: 输入层权重矩阵
% :param b1: 输入层的偏置向量
% :param W2: 隐藏层的权重矩阵
% :param b2: 隐藏层的偏置向量
% :param lr: 学习率
%
% :return loss: 损失函数
% :return y: 网络输出单元
% :return W1: 梯度下降后的输入层权重矩阵
% :return b1: 梯度下降后的输入层的偏置向量
% :return W2: 梯度下降后的隐藏层的权重矩阵
% :return b2: 梯度下降后的隐藏层的偏置向量

% 程序实现1：人工神经网络的的前向传播================================



% 程序结束1==================================================================

% 损失函数计算
softmax_loss = softmax_forward(y, train_label);  % softmax损失
loss = softmax_loss;  % 总损失

% 损失函数后向传播
ds = softmax_backward(y, train_label);

% 程序实现2：人工神经网络的的后向传播===============================



% 程序结束2=================================================================

% 程序实现3：梯度下降========================================================



% 程序结束3=================================================================
